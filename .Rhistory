# Train ordinal Random Forest model with current seed value
ordinal_rf_model <- ranger(
Usefulness_score ~ . - Dialogue_ID,
data = train_no1361,
num.trees = 100,
probability = TRUE,
min.node.size = 15,
replace = FALSE,
seed = seed_value
)
# Predict Usefulness_score probabilities for validation_combined
predictions_rf_probs <- predict(ordinal_rf_model, data = validation_combined, type = "response")
# Extract predicted probabilities from predictions_rf_probs
predicted_probabilities <- predictions_rf_probs$predictions
# Convert predicted probabilities to predicted classes
predicted_classes <- apply(predicted_probabilities, 1, function(x) sum(cumsum(x) < 0.5) + 1)
# Calculate accuracy
accuracy <- mean(predicted_classes == validation_combined$Usefulness_score)
# Update best seed and accuracy if current accuracy is higher
if (accuracy > best_accuracy) {
best_seed <- seed_value
best_accuracy <- accuracy
}
}
# Print best seed and accuracy
cat("Best seed value:", best_seed, "\n")
cat("Best accuracy:", best_accuracy, "\n")
# Train ordinal Random Forest model
ordinal_rf_model <- ranger(Usefulness_score ~ . - Dialogue_ID, data = train_no1361, num.trees = 100, probability = TRUE, min.node.size = 15, replace = FALSE, seed = 114)
# Predict Usefulness_score probabilities for validation_combined using ordinal Random Forest
predictions_rf_probs <- predict(ordinal_rf_model, data = validation_combined, type = "response")
#str(predictions_rf_probs)
# Extract predicted probabilities from predictions_rf_probs
predicted_probabilities <- predictions_rf_probs$predictions
# Convert predicted probabilities to predicted classes
predicted_classes_rf <- apply(predicted_probabilities, 1, function(x) sum(cumsum(x) < 0.5) + 1)
# Measure performance of ordinal Random Forest
accuracy_rf <- mean(predicted_classes_rf == validation_combined$Usefulness_score)
cat("Accuracy of ordinal Random Forest:", accuracy_rf, "\n")
best_seed_svm <- NULL
best_accuracy_svm <- 0
# Loop over seed values from 0 to 1000
for (seed_value in 0:1000) {
# Train SVM model with current seed value
svm_model <- svm(
Usefulness_score ~ . - Dialogue_ID,
data = train_no1361,
kernel = "linear",
probability = TRUE,
scale = TRUE,
seed = seed_value
)
# Predict Usefulness_score for validation_combined
predicted_classes_svm <- predict(svm_model, newdata = validation_combined)
# Calculate accuracy
accuracy_svm <- mean(predicted_classes_svm == validation_combined$Usefulness_score)
# Update best seed and accuracy if current accuracy is higher
if (accuracy_svm > best_accuracy_svm) {
best_seed_svm <- seed_value
best_accuracy_svm <- accuracy_svm
}
}
# Print best seed and accuracy
cat("Best seed value for SVM:", best_seed_svm, "\n")
cat("Best accuracy for SVM:", best_accuracy_svm, "\n")
# Train SVM model with current seed value
svm_model <- svm(Usefulness_score ~ . - Dialogue_ID, data = train_no1361, kernel = "linear",  probability = TRUE, scale = TRUE, seed = 0)
# Predict Usefulness_score for validation_combined
predicted_classes_svm <- predict(svm_model, newdata = validation_combined)
# Measure performance
accuracy_svm <- mean(predicted_classes_svm == validation_combined$Usefulness_score)
cat("Accuracy of SVM:", accuracy_svm, "\n")
# Visualize histogram distributions of each feature in train_combined
# Feature 1: Number of request-response pairs per Dialogue_ID
hist(train_combined$request_response_pairs, main = "Histogram of Feature 1", xlab = "Number of request-response pairs", freq = TRUE)
# Feature 2: Dialogue time
hist(train_combined$dialogue_time_min, main = "Histogram of Feature 2", xlab = "Dialogue time", freq = TRUE)
# Feature 3: Number of useful words in chatbot responses
hist(train_combined$chatbot_text_amount, main = "Histogram of Feature 3", xlab = "Number of useful words", freq = TRUE)
# Feature 4: Average response time
hist(train_combined$avg_response_time_s, main = "Histogram of Feature 4", xlab = "Average response time", freq = TRUE)
# Log Transformation for Feature 3
train_new <- train_combined %>%
mutate(Feature_3_log = log(chatbot_text_amount + 1))
validation_final <- validation_combined %>%
mutate(Feature_3_log = log(chatbot_text_amount + 1))
test_final <- test_combined %>%
mutate(Feature_3_log = log(chatbot_text_amount + 1))
# Square Root Transformation for Feature 3
train_new <- train_new %>%
mutate(Feature_3_sqrt = sqrt(chatbot_text_amount))
validation_final <- validation_final %>%
mutate(Feature_3_sqrt = sqrt(chatbot_text_amount))
test_final <- test_final %>%
mutate(Feature_3_sqrt = sqrt(chatbot_text_amount))
# View transformed features
head(train_new)
# rename the columns to corresponding Features
train_final <- rename(train_new, Feature_1 = request_response_pairs, Feature_2 = dialogue_time_min, Feature_3 = chatbot_text_amount, Feature_4 = avg_response_time_s)
validation_final <- rename(validation_final, Feature_1 = request_response_pairs, Feature_2 = dialogue_time_min, Feature_3 = chatbot_text_amount, Feature_4 = avg_response_time_s)
test_final <- rename(test_final, Feature_1 = request_response_pairs, Feature_2 = dialogue_time_min, Feature_3 = chatbot_text_amount, Feature_4 = avg_response_time_s)
dim(train_final)
head(validation_final)
# Separate the columns that we don't want to standardize
non_numeric_columns <- train_final %>% dplyr::select(Dialogue_ID, Usefulness_score)
non_numeric_val <- validation_final %>% dplyr::select(Dialogue_ID, Usefulness_score)
non_numeric_test <- test_final %>% dplyr::select(Dialogue_ID, Usefulness_score)
# Standardize the numeric columns
numeric_columns <- train_final %>% dplyr::select(-Dialogue_ID, -Usefulness_score) %>% as.data.frame()
standardized_numeric_columns <- as.data.frame(scale(numeric_columns))
numeric_val <- validation_final %>% dplyr::select(-Dialogue_ID, -Usefulness_score) %>% as.data.frame()
std_numeric_val <- as.data.frame(scale(numeric_val))
numeric_test <- test_final %>% dplyr::select(-Dialogue_ID, -Usefulness_score) %>% as.data.frame()
std_numeric_test <- as.data.frame(scale(numeric_test))
# Combine the standardized numeric columns with the non-standardized columns
train_final_std <- bind_cols(non_numeric_columns, standardized_numeric_columns)
val_final_std <- bind_cols(non_numeric_val, std_numeric_val)
test_final_std <- bind_cols(non_numeric_test, std_numeric_test)
# Print the first few rows to verify
print(head(train_final_std))
# Draw the boxplot for Feature 2 (dialogue_time)
ggplot(train_final_std, aes(y = Feature_2)) +
geom_boxplot() +
ggtitle("Boxplot of Dialogue Time (Feature 2)") +
ylab("Dialogue Time")
# Tukey's method of outlier detection
Q1 <- quantile(train_final_std$Feature_2, 0.25)
Q3 <- quantile(train_final_std$Feature_2, 0.75)
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
# Filter out the outliers to create train_new
train_final_2 <- train_final_std %>%
filter(Feature_2 >= lower_bound & Feature_2 <= upper_bound)
# Display the number of rows before and after removing outliers
cat("Number of rows in original dataset:", nrow(train_final_std), "\n")
cat("Number of rows after removing outliers:", nrow(train_final_2), "\n")
# save entry with my Dialogue ID
my_data <- train_final_std %>% filter(Dialogue_ID == 1361)
my_data
# Remove entry with Dialogue_ID 1361 from train_final_std and train_final_2
train_final_std <- train_final_std %>% filter(Dialogue_ID != 1361)
train_final_2 <- train_final_2 %>% filter(Dialogue_ID != 1361)
dim(train_final_std)
dim(train_final_2)
# Train ordinal logistic regression model with different column combinations
new_model_1a <- polr(factor(Usefulness_score) ~ Feature_1 + Feature_2 + Feature_3 + Feature_4, data = train_final_2) # _log, _sqrt, Feature_1 +
# Predict on the validation set
predictions_logit_new <- predict(new_model_1a, newdata = val_final_std, type = "class")
# Measure performance of new model
accuracy_logit_new <- mean(predictions_logit_new == val_final_std$Usefulness_score)
cat("Accuracy of ordinal Logistic Regression:", accuracy_logit_new, "\n")
# Train ordinal logistic regression model with different column combinations
new_model_1b <- polr(factor(Usefulness_score) ~ Feature_1 + Feature_2 + Feature_3_sqrt + Feature_4, data = train_final_std) # _log, _sqrt, Feature_1 +
# Predict on the validation set
predictions_logit_new <- predict(new_model_1b, newdata = val_final_std, type = "class")
# Measure performance of new model
accuracy_logit_new <- mean(predictions_logit_new == val_final_std$Usefulness_score)
cat("Accuracy of ordinal Logistic Regression:", accuracy_logit_new, "\n")
# Filter rows with Dialogue_ID 1361
dialogue_1361 <- train_chat %>% filter(Dialogue_ID == 1361)
# Combine the Utterance_text for these rows
combined_utterance_text <- paste(dialogue_1361$Utterance_text, collapse = "\n")
# Print the combined Utterance_text
cat("Combined Utterance_text for Dialogue_ID 1361:\n", combined_utterance_text, "\n")
# My data from combined training dataset
data_1361 <- my_data
data_1361
# Predict on the validation set
prediction_my_data <- predict(new_model_1b, newdata = data_1361, type = "class")
# Compare prediction to ground truth
cat("Prediction value for my data:", prediction_my_data, "\n")
cat("Ground truth value for my data", data_1361$Usefulness_score)
# predict the test Usefulness_scores
test_predictions <- predict(new_model_1b, newdata = test_final_std, type = "class")
# add to test set
test_final_output <- test_final_std %>%
mutate(Usefulness_score = test_predictions)
test_final_output
# output file data
test_output <- test_final_output %>% dplyr::select(Dialogue_ID, Usefulness_score)
# output the data into .csv format
write.csv(test_output, file = "Sunil_34113339_dialogue_usefulness_test.csv", row.names = FALSE)
# Train ordinal logistic regression model with different column combinations
new_model_1a <- polr(factor(Usefulness_score) ~ Feature_2 + Feature_3 + Feature_4, data = train_final_2) # _log, _sqrt, Feature_1 +
# Predict on the validation set
predictions_logit_new <- predict(new_model_1a, newdata = val_final_std, type = "class")
# Measure performance of new model
accuracy_logit_new <- mean(predictions_logit_new == val_final_std$Usefulness_score)
cat("Accuracy of ordinal Logistic Regression:", accuracy_logit_new, "\n")
# Train ordinal logistic regression model with different column combinations
new_model_1a <- polr(factor(Usefulness_score) ~ Feature_2 + Feature_3_log + Feature_4, data = train_final_2) # _log, _sqrt, Feature_1 +
# Predict on the validation set
predictions_logit_new <- predict(new_model_1a, newdata = val_final_std, type = "class")
# Measure performance of new model
accuracy_logit_new <- mean(predictions_logit_new == val_final_std$Usefulness_score)
cat("Accuracy of ordinal Logistic Regression:", accuracy_logit_new, "\n")
# Train ordinal logistic regression model with different column combinations
new_model_1a <- polr(factor(Usefulness_score) ~ Feature_2 + Feature_3_sqrt + Feature_4, data = train_final_2) # _log, _sqrt, Feature_1 +
# Predict on the validation set
predictions_logit_new <- predict(new_model_1a, newdata = val_final_std, type = "class")
# Measure performance of new model
accuracy_logit_new <- mean(predictions_logit_new == val_final_std$Usefulness_score)
cat("Accuracy of ordinal Logistic Regression:", accuracy_logit_new, "\n")
# Train ordinal logistic regression model with different column combinations
new_model_1a <- polr(factor(Usefulness_score) ~ Feature_1 + Feature_2 + Feature_3_sqrt + Feature_4, data = train_final_2) # _log, _sqrt, Feature_1 +
# Predict on the validation set
predictions_logit_new <- predict(new_model_1a, newdata = val_final_std, type = "class")
# Measure performance of new model
accuracy_logit_new <- mean(predictions_logit_new == val_final_std$Usefulness_score)
cat("Accuracy of ordinal Logistic Regression:", accuracy_logit_new, "\n")
# Train ordinal logistic regression model with different column combinations
new_model_1a <- polr(factor(Usefulness_score) ~ Feature_1 + Feature_2 + Feature_3_log + Feature_4, data = train_final_2) # _log, _sqrt, Feature_1 +
# Predict on the validation set
predictions_logit_new <- predict(new_model_1a, newdata = val_final_std, type = "class")
# Measure performance of new model
accuracy_logit_new <- mean(predictions_logit_new == val_final_std$Usefulness_score)
cat("Accuracy of ordinal Logistic Regression:", accuracy_logit_new, "\n")
# Train ordinal logistic regression model with different column combinations
new_model_1a <- polr(factor(Usefulness_score) ~ Feature_1 + Feature_2 + Feature_3 + Feature_4, data = train_final_2) # _log, _sqrt, Feature_1 +
# Predict on the validation set
predictions_logit_new <- predict(new_model_1a, newdata = val_final_std, type = "class")
# Measure performance of new model
accuracy_logit_new <- mean(predictions_logit_new == val_final_std$Usefulness_score)
cat("Accuracy of ordinal Logistic Regression:", accuracy_logit_new, "\n")
# Convert user_created_at to date-time
olympics_data$user_created_at <- dmy_hm(olympics_data$user_created_at)
# typeof(olympics_data$user_created_at)
# Extract year from user_created_at
olympics_data$year <- lubridate::year(olympics_data$user_created_at)
head(olympics_data)
# Count the number of Twitter accounts created each year
account_creation_counts <- olympics_data %>%
group_by(year) %>%
summarise(no_accounts = n())
account_creation_counts
# Plotting the bar chart
ggplot(account_creation_counts, aes(x = year, y = no_accounts)) +
geom_bar(stat = "identity", fill = "grey") +
geom_text(aes(label = no_accounts), vjust = -0.5, size = 3) +
labs(title = "Number of Twitter Accounts Created Across Different Years",
x = "Year",
y = "Number of Accounts") +
theme_minimal()
# Filter for accounts created after 2010
filtered_tweets <- olympics_data %>%
filter(year > 2010)
# Calculate the average number of user_followers for each year
avg_followers_count_by_year <- filtered_tweets %>%
group_by(year) %>%
summarise(avg_followers = mean(user_followers, na.rm = TRUE))
# Plotting the bar chart
ggplot(avg_followers_count_by_year, aes(x = year, y = avg_followers)) +
geom_bar(stat = "identity", fill = "grey") +
geom_text(aes(label = avg_followers), vjust = -0.5, size = 3) +
labs(title = "Average Number of Followers for Accounts Created After 2010",
x = "Year",
y = "Average Number of Followers") +
theme_minimal()
# Filter for accounts created after 2010
filtered_tweets <- olympics_data %>%
filter(year > 2010)
# Calculate the average number of user_followers for each year
avg_followers_count_by_year <- filtered_tweets %>%
group_by(year) %>%
summarise(avg_followers = mean(user_followers, na.rm = TRUE))
# Plotting the bar chart
ggplot(avg_followers_count_by_year, aes(x = year, y = avg_followers)) +
geom_bar(stat = "identity", fill = "grey") +
geom_text(aes(label = round(avg_followers, 2)), vjust = -0.5, size = 3) +
labs(title = "Average Number of Followers for Accounts Created After 2010",
x = "Year",
y = "Average Number of Followers") +
theme_minimal()
# Count the occurrences of different location values
unique_location_counts <- olympics_data %>%
group_by(user_location) %>%
summarise(count = n(), na.rm=TRUE) %>%
arrange(desc(count))
# Display the top 10 most frequent location values
top_10_locs <- unique_location_counts %>%
top_n(10, wt = count)
print(top_10_locs)
# Count the occurrences of different location values
unique_location_counts <- olympics_data %>%
group_by(user_location, na.rm=TRUE) %>%
summarise(count = n()) %>%
arrange(desc(count))
# Display the top 10 most frequent location values
top_10_locs <- unique_location_counts %>%
top_n(10, wt = count)
print(top_10_locs)
# Count the occurrences of different location values
unique_location_counts <- olympics_data %>%
filter(!is.na(user_location)) %>%
group_by(user_location) %>%
summarise(count = n()) %>%
arrange(desc(count))
# Display the top 10 most frequent location values
top_10_locs <- unique_location_counts %>%
top_n(10, wt = count)
print(top_10_locs)
# Count the occurrences of different location values
unique_location_counts <- olympics_data %>%
filter(!is.na(user_location)) %>%
group_by(user_location) %>%
summarise(count = n()) %>%
arrange(desc(count))
# Display the top 10 most frequent location values
top_10_locs <- unique_location_counts %>%
top_n(10, wt = count)
print(top_10_locs)
print(sum(top_10_locs$count))
# Count the occurrences of different location values
unique_location_counts <- olympics_data %>%
filter(!is.na(user_location)) %>%
group_by(user_location) %>%
summarise(count = n()) %>%
arrange(desc(count))
# Display the top 10 most frequent location values
top_10_locs <- unique_location_counts %>%
top_n(10, wt = count)
print(top_10_locs)
cat("The no. of tweets associated with top 10 most frequent location values:", sum(top_10_locs$count))
olympics_2.2 <- olympics_data
# Calculate the length of each tweet
olympics_2.2$text_length <- nchar(olympics_2.2$text)
head(olympics_2.2)
# Define the tweet size ranges
tweet_size_ranges <- c(1, 40, 80, 120, 160, 200, 240, Inf)
# Define the tweet size labels
tweet_size_labels <- c("[1, 40]", "[41, 80]", "[81, 120]", "[121, 160]", "[161, 200]", "[201, 240]", ">= 241")
# Categorize the tweets into different tweet size ranges
olympics_2.2$tweet_size <- cut(olympics_2.2$text_length, breaks = tweet_size_ranges, labels = tweet_size_labels, include.lowest = TRUE)
# Count the number of tweets for each tweet size
tweetCnts_per_tsCat <- olympics_2.2 %>%
group_by(tweet_size) %>%
summarise(no_tweets = n())
# Plotting the bar chart
ggplot(tweetCnts_per_tsCat, aes(x = tweet_size, y = no_tweets)) +
geom_bar(stat = "identity", fill = "grey") +
geom_text(aes(label = no_tweets), vjust = -0.5, size = 3) +
labs(title = "Number of Tweets by Text Length",
x = "Text Length",
y = "Number of Tweets") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Define a regular expression pattern to match usernames
username_catch <- "@\\w+\\b"
# Count the number of tweets containing at least one username mention
tweets_mentioning_username <- olympics_data %>%
filter(str_detect(text, username_catch)) %>%
nrow()
cat("Number of tweets tagging atleast 1 user-account:", tweets_mentioning_username, "\n")
# Function to extract unique usernames from a tweet
extract_unique_usernames <- function(tweet_text) {
usernames <- str_extract_all(tweet_text, username_catch)[[1]]
unique_usernames <- unique(usernames)
return(unique_usernames)
}
# Count the number of tweets containing at least three different usernames
tweets_3_mentions <- olympics_data %>%
filter(map_int(text, ~ length(extract_unique_usernames(.x))) >= 3) %>%
nrow()
cat("Number of tweets containing tagging atleast 3 different user-accounts:", tweets_3_mentions, "\n")
# Install necessary packages
if (!require(tm)) install.packages("tm")
if (!require(SnowballC)) install.packages("SnowballC")
if (!require(Matrix)) install.packages("Matrix")
if (!require(tokenizers)) install.packages("tokenizers")
if (!require(udpipe)) install.packages("udpipe")
# Load required libraries
library(tm)
library(SnowballC)
library(Matrix)
library(tokenizers) # For tokenization
library(udpipe)    # For lemmatization
# Sample the data to 10% of the original dataset size
sampled_tweets <- olympics_data %>%
sample_frac(0.1)
# Remove rows with null or NA values in the 'language' column
tweets_with_lang <- sampled_tweets %>%
filter(!is.na(language) & language != "")
# Count the number of rows removed
rows_removed <- nrow(olympics_data) - nrow(tweets_with_lang)
cat("Number of rows removed due to null in 'language' or sampling:", rows_removed, "\n")
# Install necessary packages
if (!require(tm)) install.packages("tm")
if (!require(SnowballC)) install.packages("SnowballC")
if (!require(Matrix)) install.packages("Matrix")
if (!require(tokenizers)) install.packages("tokenizers")
# Load required libraries
library(tm)
library(SnowballC)
library(Matrix)
library(tokenizers) # For tokenization
# Sample the data to 10% of the original dataset size
sampled_tweets <- olympics_data %>%
sample_frac(0.1)
# Remove rows with null or NA values in the 'language' column
tweets_with_lang <- sampled_tweets %>%
filter(!is.na(language) & language != "")
# Count the number of rows removed
rows_removed <- nrow(olympics_data) - nrow(tweets_with_lang)
cat("Number of rows removed due to null in 'language' or sampling:", rows_removed, "\n")
# Tokenize the text of all tweets
corpus <- Corpus(VectorSource(tweets_with_lang$text))
# Text processing
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
# Convert the corpus to a document-term matrix
dtm_before <- DocumentTermMatrix(corpus)
# Convert the document-term matrix to a matrix
before_matrix <- as.matrix(dtm_before, "sparseMatrix")
# Calculate the term frequencies before removing stopwords
term_freq_before <- colSums(before_matrix)
# Sort the term frequencies in descending order
sorted_tf_before <- sort(term_freq_before, decreasing = TRUE)
# Identify the top 20 most frequent terms before removing stopwords
top20_tf_before <- head(sorted_tf_before, 20)
# Print the top 20 most frequent terms before removing stopwords
print("Top 20 most frequent terms in text (without stopword removal):")
print(top20_tf_before)
# Remove stopwords based on the language specified - en
corpus <- tm_map(corpus, removeWords, stopwords("en"))
# Convert the corpus to a document-term matrix
dtm_after <- DocumentTermMatrix(corpus)
# Convert the document-term matrix to a matrix
after_matrix <- as.matrix(dtm_after, "sparseMatrix")
# Calculate the term frequencies after removing stopwords
term_freq_after <- colSums(after_matrix)
# Sort the term frequencies in descending order
sorted_tf_after <- sort(term_freq_after, decreasing = TRUE)
# Identify the top 20 most frequent terms after removing stopwords
top20_tf_after <- head(sorted_tf_after, 20)
# Print the top 20 most frequent terms after removing stopwords
print("\nTop 20 most frequent terms in text (after stopword removal):")
print(top20_tf_after)
# Install necessary packages
if (!require(tm)) install.packages("tm")
if (!require(SnowballC)) install.packages("SnowballC")
if (!require(Matrix)) install.packages("Matrix")
if (!require(tokenizers)) install.packages("tokenizers")
# Load required libraries
library(tm)
library(SnowballC)
library(Matrix)
library(tokenizers) # For tokenization
# Sample the data to 10% of the original dataset size
set.seed(510)
sampled_tweets <- olympics_data %>%
sample_frac(0.1)
# Remove rows with null or NA values in the 'language' column
tweets_with_lang <- sampled_tweets %>%
filter(!is.na(language) & language != "")
# Count the number of rows removed
rows_removed <- nrow(olympics_data) - nrow(tweets_with_lang)
cat("Number of rows removed due to null in 'language' or sampling:", rows_removed, "\n")
# Tokenize the text of all tweets
corpus <- Corpus(VectorSource(tweets_with_lang$text))
# Text processing
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
# Convert the corpus to a document-term matrix
dtm_before <- DocumentTermMatrix(corpus)
# Convert the document-term matrix to a matrix
before_matrix <- as.matrix(dtm_before, "sparseMatrix")
# Calculate the term frequencies before removing stopwords
term_freq_before <- colSums(before_matrix)
# Sort the term frequencies in descending order
sorted_tf_before <- sort(term_freq_before, decreasing = TRUE)
# Identify the top 20 most frequent terms before removing stopwords
top20_tf_before <- head(sorted_tf_before, 20)
# Print the top 20 most frequent terms before removing stopwords
print("Top 20 most frequent terms in text (without stopword removal):")
print(top20_tf_before)
# Remove stopwords based on the language specified - en
corpus <- tm_map(corpus, removeWords, stopwords("en"))
# Convert the corpus to a document-term matrix
dtm_after <- DocumentTermMatrix(corpus)
# Convert the document-term matrix to a matrix
after_matrix <- as.matrix(dtm_after, "sparseMatrix")
# Calculate the term frequencies after removing stopwords
term_freq_after <- colSums(after_matrix)
# Sort the term frequencies in descending order
sorted_tf_after <- sort(term_freq_after, decreasing = TRUE)
# Identify the top 20 most frequent terms after removing stopwords
top20_tf_after <- head(sorted_tf_after, 20)
# Print the top 20 most frequent terms after removing stopwords
print("\nTop 20 most frequent terms in text (after stopword removal):")
print(top20_tf_after)
# Plot boxplots for request_response_pairs of group_1_2 and group_4_5
ggplot() +
geom_boxplot(data = group_1_2, aes(x = "1-2", y = request_response_pairs, fill = "orange"), position = position_dodge(width = 0.8)) +
geom_boxplot(data = group_4_5, aes(x = "4-5", y = request_response_pairs, fill = "lightgreen"), position = position_dodge(width = 0.8)) +
labs(x = "Usefulness Score", y = "Request-Response Pairs", title = "Distribution of Request-Response Pairs for Usefulness Scores") +
theme_minimal() +
scale_fill_manual(values = c("orange", "lightgreen")) +
theme(legend.position = "none")
# Plot boxplots for dialogue_time of group_1_2 and group_4_5
ggplot() +
geom_boxplot(data = group_1_2, aes(x = "1-2", y = dialogue_time_min, fill = "orange"), position = position_dodge(width = 0.8)) +
geom_boxplot(data = group_4_5, aes(x = "4-5", y = dialogue_time_min, fill = "lightgreen"), position = position_dodge(width = 0.8)) +
labs(x = "Usefulness Score", y = "Dialogue Time (minutes)", title = "Distribution of Dialogue Time for Usefulness Scores") +
theme_minimal() +
scale_fill_manual(values = c("orange", "lightgreen")) +
theme(legend.position = "none")
shiny::runApp('MDS Sem 2/FIT5147 - Data Exploration and Visualisation/Assignments/Data Visualisation Project (DVP)/R Code')
setwd("C:/Users/anish/Documents/MDS Sem 2/FIT5147 - Data Exploration and Visualisation/Assignments/Data Visualisation Project (DVP)/AnishSunil_34113339_Code")
library(shiny); runApp('5147_DVP_AnishS.R')
